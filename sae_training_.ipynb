{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from hook_manager import HookManager\n",
    "from data_handling import load_tinystories_data\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roneneldan/TinyStories-33M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-3): 4 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_tinystories_data('data/tinystories_val.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "layers_pre_attn = []\n",
    "\n",
    "with HookManager(model) as hook_manager:\n",
    "    for layer in range(model.config.num_layers):\n",
    "        layers_pre_attn.append(hook_manager.attach_residstream_hook(layer=layer))\n",
    "    for idx, story in enumerate(data[:100]):\n",
    "        print(idx)\n",
    "        tokenized = tokenizer(story, return_tensors='pt')\n",
    "        model.forward(tokenized.input_ids)\n",
    "\n",
    "all_resids = torch.concat([torch.concat(layer) for layer in layers_pre_attn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2_resids = torch.concat(layers_pre_attn[2])\n",
    "ds = TensorDataset(layer_2_resids)\n",
    "loader = DataLoader(ds, shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae import SaeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SaeTrainer(\n",
    "    input_size=768,\n",
    "    hidden_size=768*16,\n",
    "    k=32,\n",
    "    learning_rate=0.001,\n",
    "    device='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\t\t0.5986819863319397\n",
      "0.1\t\t0.586446225643158\n",
      "0.2\t\t0.5844685435295105\n",
      "0.3\t\t0.5697855353355408\n",
      "0.4\t\t0.5604385733604431\n",
      "0.5\t\t0.5367621779441833\n",
      "0.6\t\t0.5444661378860474\n",
      "0.7\t\t0.536171019077301\n",
      "0.8\t\t0.519108772277832\n",
      "0.9\t\t0.5086182355880737\n",
      "0.10\t\t0.4949188232421875\n",
      "0.11\t\t0.5010320544242859\n",
      "0.12\t\t0.4603680372238159\n",
      "0.13\t\t0.46044185757637024\n",
      "0.14\t\t0.4428839385509491\n",
      "0.15\t\t0.44240161776542664\n",
      "0.16\t\t0.4468660056591034\n",
      "0.17\t\t0.40532755851745605\n",
      "0.18\t\t0.3950551748275757\n",
      "0.19\t\t0.39205053448677063\n",
      "0.20\t\t0.3803686201572418\n",
      "0.21\t\t0.38708925247192383\n",
      "0.22\t\t0.3510204255580902\n",
      "0.23\t\t0.3228052854537964\n",
      "0.24\t\t0.30815979838371277\n",
      "0.25\t\t0.31131234765052795\n",
      "0.26\t\t0.3229897916316986\n",
      "0.27\t\t0.2750348448753357\n",
      "0.28\t\t0.3065382242202759\n",
      "0.29\t\t0.2948538064956665\n",
      "0.30\t\t0.2720610499382019\n",
      "0.31\t\t0.286416620016098\n",
      "0.32\t\t0.26694604754447937\n",
      "0.33\t\t0.2803826928138733\n",
      "0.34\t\t0.26811525225639343\n",
      "0.35\t\t0.24681459367275238\n",
      "0.36\t\t0.2576916217803955\n",
      "0.37\t\t0.2553018033504486\n",
      "0.38\t\t0.2528240978717804\n",
      "0.39\t\t0.2773647904396057\n",
      "0.40\t\t0.2649398744106293\n",
      "0.41\t\t0.23617148399353027\n",
      "0.42\t\t0.24237032234668732\n",
      "0.43\t\t0.23756621778011322\n",
      "0.44\t\t0.27548184990882874\n",
      "0.45\t\t0.2693925201892853\n",
      "0.46\t\t0.2807825803756714\n",
      "0.47\t\t0.24123020470142365\n",
      "0.48\t\t0.2545623779296875\n",
      "0.49\t\t0.24990497529506683\n",
      "0.50\t\t0.2473994940519333\n",
      "0.51\t\t0.24454224109649658\n",
      "0.52\t\t0.25961747765541077\n",
      "0.53\t\t0.25647082924842834\n",
      "0.54\t\t0.2384054809808731\n",
      "0.55\t\t0.1985291838645935\n",
      "0.56\t\t0.23745502531528473\n",
      "0.57\t\t0.22455166280269623\n",
      "0.58\t\t0.211788609623909\n",
      "0.59\t\t0.22869998216629028\n",
      "0.60\t\t0.19575190544128418\n",
      "0.61\t\t0.277849942445755\n",
      "0.62\t\t0.22553332149982452\n",
      "0.63\t\t0.24061506986618042\n",
      "0.64\t\t0.22243119776248932\n",
      "0.65\t\t0.23459278047084808\n",
      "0.66\t\t0.24413108825683594\n",
      "0.67\t\t0.21960444748401642\n",
      "0.68\t\t0.23588521778583527\n",
      "0.69\t\t0.21687157452106476\n",
      "0.70\t\t0.23573346436023712\n",
      "0.71\t\t0.20078633725643158\n",
      "0.72\t\t0.2505480945110321\n",
      "0.73\t\t0.20103831589221954\n",
      "0.74\t\t0.2602589428424835\n",
      "0.75\t\t0.20144407451152802\n",
      "0.76\t\t0.21522970497608185\n",
      "0.77\t\t0.22748470306396484\n",
      "0.78\t\t0.20653824508190155\n",
      "0.79\t\t0.20120103657245636\n",
      "0.80\t\t0.1893600970506668\n",
      "0.81\t\t0.2238265424966812\n",
      "0.82\t\t0.20285826921463013\n",
      "0.83\t\t0.18742932379245758\n",
      "0.84\t\t0.23322220146656036\n",
      "0.85\t\t0.22745941579341888\n",
      "0.86\t\t0.23475416004657745\n",
      "0.87\t\t0.18520613014698029\n",
      "0.88\t\t0.2022034376859665\n",
      "0.89\t\t0.2240075320005417\n",
      "0.90\t\t0.18473899364471436\n",
      "0.91\t\t0.2183626890182495\n",
      "0.92\t\t0.210381880402565\n",
      "0.93\t\t0.2145136147737503\n",
      "0.94\t\t0.17787744104862213\n",
      "0.95\t\t0.2148517221212387\n",
      "0.96\t\t0.20647598803043365\n",
      "0.97\t\t0.19379109144210815\n",
      "0.98\t\t0.22006545960903168\n",
      "0.99\t\t0.22646260261535645\n",
      "0.100\t\t0.22491002082824707\n",
      "0.101\t\t0.2114802747964859\n",
      "0.102\t\t0.1834440678358078\n",
      "0.103\t\t0.18774940073490143\n",
      "0.104\t\t0.20689506828784943\n",
      "0.105\t\t0.1869249939918518\n",
      "0.106\t\t0.18321293592453003\n",
      "0.107\t\t0.20382381975650787\n",
      "0.108\t\t0.16605262458324432\n",
      "0.109\t\t0.2144766002893448\n",
      "0.110\t\t0.20046357810497284\n",
      "0.111\t\t0.23116642236709595\n",
      "0.112\t\t0.19306324422359467\n",
      "0.113\t\t0.19610047340393066\n",
      "0.114\t\t0.2078331708908081\n",
      "0.115\t\t0.21241478621959686\n",
      "0.116\t\t0.2234705537557602\n",
      "0.117\t\t0.22891823947429657\n",
      "0.118\t\t0.17975503206253052\n",
      "0.119\t\t0.20188723504543304\n",
      "0.120\t\t0.18234783411026\n",
      "0.121\t\t0.1878861039876938\n",
      "0.122\t\t0.18341858685016632\n",
      "0.123\t\t0.19242389500141144\n",
      "0.124\t\t0.23342840373516083\n",
      "0.125\t\t0.1930662840604782\n",
      "0.126\t\t0.17850059270858765\n",
      "0.127\t\t0.1984885185956955\n",
      "0.128\t\t0.2071560025215149\n",
      "0.129\t\t0.20098328590393066\n",
      "0.130\t\t0.19269226491451263\n",
      "0.131\t\t0.20509283244609833\n",
      "0.132\t\t0.17118577659130096\n",
      "0.133\t\t0.18405954539775848\n",
      "0.134\t\t0.21030525863170624\n",
      "0.135\t\t0.22525298595428467\n",
      "0.136\t\t0.20013190805912018\n",
      "0.137\t\t0.22409437596797943\n",
      "0.138\t\t0.23176799714565277\n",
      "0.139\t\t0.1952916383743286\n",
      "0.140\t\t0.1358124017715454\n",
      "0.141\t\t0.1816709190607071\n",
      "0.142\t\t0.19297395646572113\n",
      "0.143\t\t0.1714322417974472\n",
      "0.144\t\t0.15983639657497406\n",
      "0.145\t\t0.2194298952817917\n",
      "0.146\t\t0.188121035695076\n",
      "0.147\t\t0.19620001316070557\n",
      "0.148\t\t0.2120111733675003\n",
      "0.149\t\t0.17029917240142822\n",
      "0.150\t\t0.20367000997066498\n",
      "0.151\t\t0.21270667016506195\n",
      "0.152\t\t0.1760597974061966\n",
      "0.153\t\t0.16359001398086548\n",
      "0.154\t\t0.2131878137588501\n",
      "0.155\t\t0.16998177766799927\n",
      "0.156\t\t0.17016386985778809\n",
      "0.157\t\t0.16495294868946075\n",
      "0.158\t\t0.15259043872356415\n",
      "0.159\t\t0.19307196140289307\n",
      "0.160\t\t0.16898150742053986\n",
      "0.161\t\t0.16434408724308014\n",
      "0.162\t\t0.19172222912311554\n",
      "0.163\t\t0.1993996500968933\n",
      "0.164\t\t0.17800171673297882\n",
      "0.165\t\t0.19956432282924652\n",
      "0.166\t\t0.19170613586902618\n",
      "0.167\t\t0.19928783178329468\n",
      "0.168\t\t0.16688092052936554\n",
      "0.169\t\t0.18392634391784668\n",
      "0.170\t\t0.1828085333108902\n",
      "0.171\t\t0.2032734900712967\n",
      "0.172\t\t0.19055618345737457\n",
      "0.173\t\t0.18327395617961884\n",
      "0.174\t\t0.2026553750038147\n",
      "0.175\t\t0.2148350328207016\n",
      "0.176\t\t0.21677625179290771\n",
      "0.177\t\t0.16776005923748016\n",
      "0.178\t\t0.19242097437381744\n",
      "0.179\t\t0.16939972341060638\n",
      "0.180\t\t0.18807901442050934\n",
      "0.181\t\t0.17954380810260773\n",
      "0.182\t\t0.19028466939926147\n",
      "0.183\t\t0.16428960859775543\n",
      "0.184\t\t0.18655771017074585\n",
      "0.185\t\t0.1835852414369583\n",
      "0.186\t\t0.20792686939239502\n",
      "0.187\t\t0.1705300658941269\n",
      "0.188\t\t0.20258885622024536\n",
      "0.189\t\t0.21220894157886505\n",
      "0.190\t\t0.1587037593126297\n",
      "0.191\t\t0.17719192802906036\n",
      "0.192\t\t0.20484425127506256\n",
      "0.193\t\t0.18363738059997559\n",
      "0.194\t\t0.17067396640777588\n",
      "0.195\t\t0.22784824669361115\n",
      "0.196\t\t0.15520475804805756\n",
      "0.197\t\t0.22968043386936188\n",
      "0.198\t\t0.20448486506938934\n",
      "0.199\t\t0.17718911170959473\n",
      "0.200\t\t0.18894164264202118\n",
      "0.201\t\t0.18430602550506592\n",
      "0.202\t\t0.1603563278913498\n",
      "0.203\t\t0.22487406432628632\n",
      "0.204\t\t0.19238509237766266\n",
      "0.205\t\t0.1612163484096527\n",
      "0.206\t\t0.17502351105213165\n",
      "0.207\t\t0.17284244298934937\n",
      "0.208\t\t0.17813420295715332\n",
      "0.209\t\t0.16025134921073914\n",
      "0.210\t\t0.1904837042093277\n",
      "0.211\t\t0.1623350977897644\n",
      "0.212\t\t0.15174758434295654\n",
      "0.213\t\t0.1769552081823349\n",
      "0.214\t\t0.16550906002521515\n",
      "0.215\t\t0.19442515075206757\n",
      "0.216\t\t0.15562854707241058\n",
      "0.217\t\t0.19670169055461884\n",
      "0.218\t\t0.1784248799085617\n",
      "0.219\t\t0.185437873005867\n",
      "0.220\t\t0.17265774309635162\n",
      "0.221\t\t0.1603073924779892\n",
      "0.222\t\t0.16816742718219757\n",
      "0.223\t\t0.1580701619386673\n",
      "0.224\t\t0.149396151304245\n",
      "0.225\t\t0.18085086345672607\n",
      "0.226\t\t0.2205173522233963\n",
      "0.227\t\t0.17378294467926025\n",
      "0.228\t\t0.17075373232364655\n",
      "0.229\t\t0.15884260833263397\n",
      "0.230\t\t0.1897573322057724\n",
      "0.231\t\t0.16107387840747833\n",
      "0.232\t\t0.18496078252792358\n",
      "0.233\t\t0.18096162378787994\n",
      "0.234\t\t0.16914905607700348\n",
      "0.235\t\t0.19663523137569427\n",
      "0.236\t\t0.1771029233932495\n",
      "0.237\t\t0.1690959334373474\n",
      "0.238\t\t0.18928475677967072\n",
      "0.239\t\t0.1665085107088089\n",
      "0.240\t\t0.19659392535686493\n",
      "0.241\t\t0.18201865255832672\n",
      "0.242\t\t0.2113279551267624\n",
      "0.243\t\t0.2227197289466858\n",
      "0.244\t\t0.18068556487560272\n",
      "0.245\t\t0.1864006072282791\n",
      "0.246\t\t0.15166744589805603\n",
      "0.247\t\t0.16110454499721527\n",
      "0.248\t\t0.15756048262119293\n",
      "0.249\t\t0.1735147386789322\n",
      "0.250\t\t0.163791224360466\n",
      "0.251\t\t0.1733492761850357\n",
      "0.252\t\t0.15342886745929718\n",
      "0.253\t\t0.18020103871822357\n",
      "0.254\t\t0.16253556311130524\n",
      "0.255\t\t0.15490134060382843\n",
      "0.256\t\t0.1589980125427246\n",
      "0.257\t\t0.17377294600009918\n",
      "0.258\t\t0.16664306819438934\n",
      "0.259\t\t0.17943286895751953\n",
      "0.260\t\t0.17727424204349518\n",
      "0.261\t\t0.18203061819076538\n",
      "0.262\t\t0.173039510846138\n",
      "0.263\t\t0.17672036588191986\n",
      "0.264\t\t0.1492110788822174\n",
      "0.265\t\t0.20801883935928345\n",
      "0.266\t\t0.16533274948596954\n",
      "0.267\t\t0.15807993710041046\n",
      "0.268\t\t0.15615622699260712\n",
      "0.269\t\t0.1887606382369995\n",
      "0.270\t\t0.16217650473117828\n",
      "0.271\t\t0.1681618094444275\n",
      "0.272\t\t0.17364884912967682\n",
      "0.273\t\t0.1655484288930893\n",
      "0.274\t\t0.16396015882492065\n",
      "0.275\t\t0.1487765908241272\n",
      "0.276\t\t0.1758796125650406\n",
      "0.277\t\t0.1404537707567215\n",
      "0.278\t\t0.16026075184345245\n",
      "0.279\t\t0.14878052473068237\n",
      "0.280\t\t0.15881691873073578\n",
      "0.281\t\t0.17657721042633057\n",
      "0.282\t\t0.14913003146648407\n",
      "0.283\t\t0.15441031754016876\n",
      "0.284\t\t0.15775321424007416\n",
      "0.285\t\t0.16897183656692505\n",
      "0.286\t\t0.1664292961359024\n",
      "0.287\t\t0.15834806859493256\n",
      "0.288\t\t0.1987902671098709\n",
      "0.289\t\t0.1449928730726242\n",
      "0.290\t\t0.16485263407230377\n",
      "0.291\t\t0.1658422350883484\n",
      "0.292\t\t0.15914107859134674\n",
      "0.293\t\t0.17859798669815063\n",
      "0.294\t\t0.1796358823776245\n",
      "0.295\t\t0.15194521844387054\n",
      "0.296\t\t0.15490655601024628\n",
      "0.297\t\t0.15474073588848114\n",
      "0.298\t\t0.16097979247570038\n",
      "0.299\t\t0.16036374866962433\n",
      "0.300\t\t0.17485882341861725\n",
      "0.301\t\t0.18087226152420044\n",
      "0.302\t\t0.15010063350200653\n",
      "0.303\t\t0.1660468429327011\n",
      "0.304\t\t0.15793722867965698\n",
      "0.305\t\t0.13416162133216858\n",
      "0.306\t\t0.13737094402313232\n",
      "0.307\t\t0.15894661843776703\n",
      "0.308\t\t0.16957606375217438\n",
      "0.309\t\t0.18270444869995117\n",
      "0.310\t\t0.1667170524597168\n",
      "0.311\t\t0.1384822279214859\n",
      "0.312\t\t0.16408465802669525\n",
      "0.313\t\t0.16302569210529327\n",
      "0.314\t\t0.167610764503479\n",
      "0.315\t\t0.19039380550384521\n",
      "0.316\t\t0.1626400500535965\n",
      "0.317\t\t0.13640688359737396\n",
      "0.318\t\t0.1435718983411789\n",
      "0.319\t\t0.15866276621818542\n",
      "0.320\t\t0.156388059258461\n",
      "0.321\t\t0.15892109274864197\n",
      "0.322\t\t0.162523090839386\n",
      "0.323\t\t0.17336051166057587\n",
      "0.324\t\t0.1597740203142166\n",
      "0.325\t\t0.17185096442699432\n",
      "0.326\t\t0.13373582065105438\n",
      "0.327\t\t0.13768038153648376\n",
      "0.328\t\t0.160756453871727\n",
      "0.329\t\t0.15928733348846436\n",
      "0.330\t\t0.12658147513866425\n",
      "0.331\t\t0.15008269250392914\n",
      "0.332\t\t0.15263108909130096\n",
      "0.333\t\t0.11519526690244675\n",
      "0.334\t\t0.14402349293231964\n",
      "0.335\t\t0.1321975737810135\n",
      "0.336\t\t0.12649042904376984\n",
      "0.337\t\t0.16240431368350983\n",
      "0.338\t\t0.16948138177394867\n",
      "0.339\t\t0.15754936635494232\n",
      "0.340\t\t0.1264386773109436\n",
      "0.341\t\t0.15305322408676147\n",
      "0.342\t\t0.1742565631866455\n",
      "0.343\t\t0.15582990646362305\n",
      "0.344\t\t0.14852717518806458\n",
      "0.345\t\t0.17872333526611328\n",
      "0.346\t\t0.14462053775787354\n",
      "0.347\t\t0.15938429534435272\n",
      "0.348\t\t0.13815990090370178\n",
      "0.349\t\t0.1930568814277649\n",
      "0.350\t\t0.13534462451934814\n",
      "0.351\t\t0.14101643860340118\n",
      "0.352\t\t0.18756675720214844\n",
      "0.353\t\t0.13298407196998596\n",
      "0.354\t\t0.1714807152748108\n",
      "0.355\t\t0.10575161129236221\n",
      "0.356\t\t0.14812679588794708\n",
      "0.357\t\t0.1778186559677124\n",
      "0.358\t\t0.16462388634681702\n",
      "0.359\t\t0.1546715497970581\n",
      "0.360\t\t0.15486803650856018\n",
      "0.361\t\t0.13958348333835602\n",
      "0.362\t\t0.15627926588058472\n",
      "0.363\t\t0.14440487325191498\n",
      "0.364\t\t0.15480084717273712\n",
      "0.365\t\t0.1607390195131302\n",
      "0.366\t\t0.14502792060375214\n",
      "0.367\t\t0.15733075141906738\n",
      "0.368\t\t0.15127336978912354\n",
      "0.369\t\t0.12631772458553314\n",
      "0.370\t\t0.16634686291217804\n",
      "0.371\t\t0.13989943265914917\n",
      "0.372\t\t0.14682771265506744\n",
      "0.373\t\t0.13550998270511627\n",
      "0.374\t\t0.1498122662305832\n",
      "0.375\t\t0.1533164083957672\n",
      "0.376\t\t0.13821589946746826\n",
      "0.377\t\t0.14520059525966644\n",
      "0.378\t\t0.1593039184808731\n",
      "0.379\t\t0.1921498328447342\n",
      "0.380\t\t0.15358184278011322\n",
      "0.381\t\t0.16352391242980957\n",
      "0.382\t\t0.14259523153305054\n",
      "0.383\t\t0.1742299348115921\n",
      "0.384\t\t0.13312457501888275\n",
      "0.385\t\t0.1686430722475052\n",
      "0.386\t\t0.14987404644489288\n",
      "0.387\t\t0.14380834996700287\n",
      "0.388\t\t0.15746985375881195\n",
      "0.389\t\t0.1497456282377243\n",
      "0.390\t\t0.15176233649253845\n",
      "0.391\t\t0.17033517360687256\n",
      "0.392\t\t0.1656065434217453\n",
      "0.393\t\t0.15714450180530548\n",
      "0.394\t\t0.13879258930683136\n",
      "0.395\t\t0.15414796769618988\n",
      "0.396\t\t0.14400143921375275\n",
      "0.397\t\t0.12416696548461914\n",
      "0.398\t\t0.11382472515106201\n",
      "0.399\t\t0.16079173982143402\n",
      "0.400\t\t0.1566668003797531\n",
      "0.401\t\t0.1387719362974167\n",
      "0.402\t\t0.15268386900424957\n",
      "0.403\t\t0.16200144588947296\n",
      "0.404\t\t0.1637779027223587\n",
      "0.405\t\t0.17668688297271729\n",
      "0.406\t\t0.15997439622879028\n",
      "0.407\t\t0.1409996598958969\n",
      "0.408\t\t0.12623465061187744\n",
      "0.409\t\t0.13335850834846497\n",
      "0.410\t\t0.14566704630851746\n",
      "0.411\t\t0.14925162494182587\n",
      "0.412\t\t0.15283463895320892\n",
      "0.413\t\t0.129788339138031\n",
      "0.414\t\t0.13182491064071655\n",
      "0.415\t\t0.15631313621997833\n",
      "0.416\t\t0.14261595904827118\n",
      "0.417\t\t0.14973394572734833\n",
      "0.418\t\t0.1404762715101242\n",
      "0.419\t\t0.1653045117855072\n",
      "0.420\t\t0.14334909617900848\n",
      "0.421\t\t0.1588403284549713\n",
      "0.422\t\t0.13738799095153809\n",
      "0.423\t\t0.17873615026474\n",
      "0.424\t\t0.1562355011701584\n",
      "0.425\t\t0.14522357285022736\n",
      "0.426\t\t0.15250885486602783\n",
      "0.427\t\t0.18078793585300446\n",
      "0.428\t\t0.15462706983089447\n",
      "0.429\t\t0.148835226893425\n",
      "0.430\t\t0.1450522541999817\n",
      "0.431\t\t0.17826467752456665\n",
      "0.432\t\t0.1398564577102661\n",
      "0.433\t\t0.1591121256351471\n",
      "0.434\t\t0.16936121881008148\n",
      "0.435\t\t0.15703274309635162\n",
      "0.436\t\t0.18663783371448517\n",
      "0.437\t\t0.1562669277191162\n",
      "0.438\t\t0.15369339287281036\n",
      "0.439\t\t0.13460548222064972\n",
      "0.440\t\t0.16689951717853546\n",
      "0.441\t\t0.13785770535469055\n",
      "0.442\t\t0.1542493999004364\n",
      "0.443\t\t0.16080869734287262\n",
      "0.444\t\t0.16934263706207275\n",
      "0.445\t\t0.14008799195289612\n",
      "0.446\t\t0.1511041224002838\n",
      "0.447\t\t0.16830997169017792\n",
      "0.448\t\t0.1612662672996521\n",
      "0.449\t\t0.15869049727916718\n",
      "0.450\t\t0.13518662750720978\n",
      "0.451\t\t0.17694872617721558\n",
      "0.452\t\t0.13858073949813843\n",
      "0.453\t\t0.16716782748699188\n",
      "0.454\t\t0.15910498797893524\n",
      "0.455\t\t0.1525236964225769\n",
      "0.456\t\t0.16199807822704315\n",
      "0.457\t\t0.1321120709180832\n",
      "0.458\t\t0.12383530288934708\n",
      "0.459\t\t0.14705361425876617\n",
      "0.460\t\t0.15475304424762726\n",
      "0.461\t\t0.14016996324062347\n",
      "0.462\t\t0.1512226015329361\n",
      "0.463\t\t0.12359738349914551\n",
      "0.464\t\t0.11329030990600586\n",
      "0.465\t\t0.17296533286571503\n",
      "0.466\t\t0.16380350291728973\n",
      "0.467\t\t0.13223595917224884\n",
      "0.468\t\t0.14648617804050446\n",
      "0.469\t\t0.13184110820293427\n",
      "0.470\t\t0.15945470333099365\n",
      "0.471\t\t0.15526176989078522\n",
      "0.472\t\t0.1256074160337448\n",
      "0.473\t\t0.11470847576856613\n",
      "0.474\t\t0.14587369561195374\n",
      "0.475\t\t0.1559617668390274\n",
      "0.476\t\t0.14520412683486938\n",
      "0.477\t\t0.16482460498809814\n",
      "0.478\t\t0.15601672232151031\n",
      "0.479\t\t0.13753308355808258\n",
      "0.480\t\t0.13342998921871185\n",
      "0.481\t\t0.14093081653118134\n",
      "0.482\t\t0.1399802267551422\n",
      "0.483\t\t0.14797687530517578\n",
      "0.484\t\t0.14270146191120148\n",
      "0.485\t\t0.16598708927631378\n",
      "0.486\t\t0.14090001583099365\n",
      "0.487\t\t0.15111511945724487\n",
      "0.488\t\t0.15062463283538818\n",
      "0.489\t\t0.15622667968273163\n",
      "0.490\t\t0.16201068460941315\n",
      "0.491\t\t0.12202835828065872\n",
      "0.492\t\t0.1679282784461975\n",
      "0.493\t\t0.13366647064685822\n",
      "0.494\t\t0.15265928208827972\n",
      "0.495\t\t0.1340135633945465\n",
      "0.496\t\t0.13338743150234222\n",
      "0.497\t\t0.14095668494701385\n",
      "0.498\t\t0.12878817319869995\n",
      "0.499\t\t0.15035170316696167\n",
      "0.500\t\t0.16658276319503784\n",
      "0.501\t\t0.12335100024938583\n",
      "0.502\t\t0.17313051223754883\n",
      "0.503\t\t0.15472419559955597\n",
      "0.504\t\t0.12617145478725433\n",
      "0.505\t\t0.1515975147485733\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m activation \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m      5\u001b[0m label \u001b[38;5;241m=\u001b[39m activation\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m----> 7\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\Code\\mechinterp\\spaceship-game\\sae.py:61\u001b[0m, in \u001b[0;36mSaeTrainer.train_step\u001b[1;34m(self, input_, labels)\u001b[0m\n\u001b[0;32m     58\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(outputs, labels)\n\u001b[0;32m     60\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\Code\\mechinterp\\mechinterp\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\Code\\mechinterp\\mechinterp\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\Code\\mechinterp\\mechinterp\\Lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\Code\\mechinterp\\mechinterp\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\Code\\mechinterp\\mechinterp\\Lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\Code\\mechinterp\\mechinterp\\Lib\\site-packages\\torch\\optim\\adam.py:379\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    378\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m--> 379\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[0;32m    382\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch_idx in range(num_epochs):\n",
    "    for idx, batch in enumerate(loader):\n",
    "        activation = batch[0].detach()\n",
    "        label = activation.detach()\n",
    "\n",
    "        loss = trainer.train_step(activation, label)\n",
    "        print(f'{epoch_idx}.{idx}\\t\\t{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    vector = trainer.model._parameters['WT'][:,i]\n",
    "\n",
    "    torch.save(vector, f'steering_vectors/test_vector_{i}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mechinterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
