{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.create_average_activation_vectors import model_setup\n",
    "from classes.hook_manager import HookManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found device: cpu\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, device = model_setup(\"roneneldan/TinyStories-1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_activations(meta_data: dict, \n",
    "    loader: DataLoader, \n",
    "    tokenizer: AutoTokenizer, \n",
    "    device: str,\n",
    "    model:AutoModelForCausalLM,\n",
    "    label_map=None\n",
    "    ) -> dict: \n",
    "\n",
    "    if label_map == None:\n",
    "        label_map = loader.dataset.label_map\n",
    "    res_stream_act_by_layer = dict()\n",
    "    activation_ds_by_layer = {\n",
    "        layer: ActivationDataset(label_map=label_map)\n",
    "        for layer in range(meta_data[\"hidden_layers\"])\n",
    "    }\n",
    "\n",
    "    if tokenizer.pad_token == None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    for ind, (text, label) in enumerate(tqdm(loader)):\n",
    "\n",
    "        if ind > 5:\n",
    "            break\n",
    "\n",
    "        \n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        with HookManager(model) as hook_manager:\n",
    "            for layer in range(meta_data[\"hidden_layers\"]):\n",
    "                res_stream_act_by_layer[layer] = hook_manager.attach_residstream_hook(\n",
    "                    layer=layer,\n",
    "                    pre_mlp=False,\n",
    "                    pythia=True if isinstance(model, GPTNeoXForCausalLM) else False\n",
    "                )\n",
    "\n",
    "            model(**tokenized)\n",
    "\n",
    "        # flattening [batch, pad_size, ...] to [tokens, ...]\n",
    "        attn_mask = tokenized.attention_mask.flatten() # [tokens]\n",
    "        label = label.unsqueeze(-1).expand(-1, tokenized.attention_mask.shape[1]).flatten() # [tokens]\n",
    "\n",
    "        for layer in range(meta_data[\"hidden_layers\"]):\n",
    "            res_stream_act_by_layer[layer] = res_stream_act_by_layer[layer][0].view(-1, meta_data[\"hidden_size\"]) # [tokens, hidden_size]\n",
    "            activation_ds_by_layer[layer].add_with_mask(res_stream_act_by_layer[layer], label, attn_mask)\n",
    "    return activation_ds_by_layer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
